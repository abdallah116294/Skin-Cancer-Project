{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2275763,
          "sourceType": "datasetVersion",
          "datasetId": 1370616
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'ham10000-oversampled-128-128:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4431066%2F7629378%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240624%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240624T211614Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D71fcb735b537b0ec122d8e36695537f5596b514a62ab133c59249616e78f7c3e80729c11cb538f187068cbd1499fd65bf93d204cff9f35e1020e679aaf4a40f89a42c54d05b800a174fffc53c950609a8ea127b9cf4e91409ac68e241aee6a2e2448174f2d723c08140792a50114c6664e6808105ec8805bcb8351501ade9c269f8a415101347cb85f43c7659b1ae9abf3084721198857870f08ab8ae182639c3daceeb8ed9050389aad2bd8d9766d3093c59d54003a78522a2fdbe85d4ca24e6fcb1d17858cb28894e5e0b69abf93c8f8f5ed93216590aba243c9ae3342a56886066b5626c5e5a066990b5c346ec987310c1371df4e1f128c0bb3a3a244507c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "K-gVRBXpS96q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a969a55b-26cd-486e-9345-83e9b6dc3257"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ham10000-oversampled-128-128, 124748051 bytes compressed\n",
            "[==================================================] 124748051 bytes downloaded\n",
            "Downloaded and uncompressed: ham10000-oversampled-128-128\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data Preprocessing</h3>\n",
        "        <p style=\"margin-left: 20px; font-size: 16px;\">\n",
        "        <ul style=\"margin-left: 20px; font-size: 16px;\">\n",
        "            <li>Loading original dataset</li>\n",
        "            <li>Oversampling the minority classes</li>\n",
        "            <li>Resizing images to 128x128</li>\n",
        "        </ul>\n",
        "        </p>\n"
      ],
      "metadata": {
        "id": "UZa1XkvYS96s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def Transform_data():\n",
        "    Image_directory  = '/kaggle/input/ham1000-segmentation-and-classification/images/'\n",
        "    labels_dir = '/kaggle/input/ham1000-segmentation-and-classification/GroundTruth.csv'\n",
        "    classes = {0:'MEL', 1:'NV', 2:'BCC', 3:'AKIEC', 4:'BKL', 5:'DF', 6:'VASC'}\n",
        "    classes_num = np.array(list(classes.keys()))\n",
        "\n",
        "    labels_df = pd.read_csv(labels_dir)\n",
        "    labels_num = np.argmax(labels_df.drop(columns=['image']).values, axis=1)\n",
        "    img_names = labels_df['image'].values\n",
        "    image_count = len(labels_df['image'].values)\n",
        "\n",
        "    #Loading Images\n",
        "    img_paths = []\n",
        "    for img in img_names:\n",
        "        img_paths.append(Image_directory + img + \".jpg\")\n",
        "\n",
        "    #train, validation, test split\n",
        "    data = list( zip(img_paths, labels_num) )\n",
        "\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "\n",
        "    val_size = int(len(data) * 0.1)\n",
        "    test_size =  int(len(data) * 0.1)\n",
        "\n",
        "    val_data = data[:val_size]\n",
        "    test_data = data[val_size:(val_size + test_size)]\n",
        "    train_data = data[(val_size + test_size):]\n",
        "\n",
        "    data = {'train_data': train_data, 'val_data': val_data, 'test_data': test_data}.items()\n",
        "\n",
        "    def resize_and_save(input_path, output_path, new_size):\n",
        "        original_image = Image.open(input_path)\n",
        "        resized_image = original_image.resize(new_size)\n",
        "        resized_image.save(output_path)\n",
        "\n",
        "    #Oversampling minority classes\n",
        "    for chunk in data:\n",
        "        chunk_name = chunk[0]\n",
        "        chunk_data = chunk[1]\n",
        "        oversample = RandomOverSampler(random_state = 3)\n",
        "\n",
        "        image_paths, labels_num = map(lambda x: list(x), zip(*chunk_data))\n",
        "        img_paths, labels_num  = oversample.fit_resample(np.array(image_paths).reshape(-1, 1), labels_num)\n",
        "\n",
        "        print(f\"Processing {chunk_name} Started..\")\n",
        "        i = 0\n",
        "        for img, label in zip(img_paths.reshape(-1), labels_num):\n",
        "            old_img_name = img.split('/')[-1]\n",
        "            new_img_name = old_img_name.split('.')[0] + str(i) + '.' + old_img_name.split('.')[1]\n",
        "            chunk_class_dir = '/kaggle/working/HAM10000_OverSampled/' + chunk_name + '/' + str(label)\n",
        "            if not os.path.exists(chunk_class_dir):\n",
        "                os.makedirs(chunk_class_dir)\n",
        "\n",
        "            output_image_path = os.path.join(chunk_class_dir, new_img_name)\n",
        "            new_size = (128, 128)\n",
        "            i += 1\n",
        "            if (i + 1) % (len(labels_num) // 10) == 0:\n",
        "                print()\n",
        "                print(\"Progress: \" + str(i) + '/' + str(len(labels_num))+ \" \"+ str(round(i /len(labels_num) * 100))+ \"%\")\n",
        "            resize_and_save(img, output_image_path, new_size)\n",
        "\n",
        "\n",
        "Transform_data()\n",
        "shutil.make_archive('HAM10000_OverSampled', 'zip', '/kaggle/working/HAM10000_OverSampled')\n",
        "print(\"\\n Data Saved\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-15T08:29:34.53753Z",
          "iopub.execute_input": "2024-02-15T08:29:34.537883Z",
          "iopub.status.idle": "2024-02-15T08:38:06.121565Z",
          "shell.execute_reply.started": "2024-02-15T08:29:34.537856Z",
          "shell.execute_reply": "2024-02-15T08:38:06.12003Z"
        },
        "trusted": true,
        "id": "yIpA75dVS96s",
        "outputId": "151fdabe-1d04-4a1b-b379-24272a739c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing train_data Started..\n\nProgress: 3749/37506 10%\n\nProgress: 7499/37506 20%\n\nProgress: 11249/37506 30%\n\nProgress: 14999/37506 40%\n\nProgress: 18749/37506 50%\n\nProgress: 22499/37506 60%\n\nProgress: 26249/37506 70%\n\nProgress: 29999/37506 80%\n\nProgress: 33749/37506 90%\n\nProgress: 37499/37506 100%\nProcessing val_data Started..\n\nProgress: 471/4725 10%\n\nProgress: 943/4725 20%\n\nProgress: 1415/4725 30%\n\nProgress: 1887/4725 40%\n\nProgress: 2359/4725 50%\n\nProgress: 2831/4725 60%\n\nProgress: 3303/4725 70%\n\nProgress: 3775/4725 80%\n\nProgress: 4247/4725 90%\n\nProgress: 4719/4725 100%\nProcessing test_data Started..\n\nProgress: 469/4704 10%\n\nProgress: 939/4704 20%\n\nProgress: 1409/4704 30%\n\nProgress: 1879/4704 40%\n\nProgress: 2349/4704 50%\n\nProgress: 2819/4704 60%\n\nProgress: 3289/4704 70%\n\nProgress: 3759/4704 80%\n\nProgress: 4229/4704 90%\n\nProgress: 4699/4704 100%\n\n Data Saved\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JY8aVoHcvIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
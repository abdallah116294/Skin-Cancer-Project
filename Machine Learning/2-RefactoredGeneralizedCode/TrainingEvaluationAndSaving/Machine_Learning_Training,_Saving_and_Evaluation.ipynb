{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1wAeV3gLq0"
      },
      "source": [
        "<h3>1. Defining functions</h3>\n",
        "<ol style=\"margin-left: 20px; font-size: 16px;\">\n",
        "    <li>\n",
        "        <p><strong>import_data()</strong></p>\n",
        "        <p style=\"margin-left: 20px; font-size: 14px;\">\n",
        "        <ul>\n",
        "            <li>Loads data into a TensorFlow dataset</li>\n",
        "            <li>Defines a normalization layer</li>\n",
        "            <li>Divides data into train, validation, and test data</li>\n",
        "        </ul>\n",
        "        </p>\n",
        "    </li>\n",
        "    <li>\n",
        "        <p><strong>model_train()</strong></p>\n",
        "        <p style=\"margin-left: 20px; font-size: 14px;\">\n",
        "        <ul>\n",
        "            <li>Trains a model with the training and validation data\n",
        "                </li>\n",
        "            <li>Saves the model along with its training history</li>\n",
        "        </ul>\n",
        "        </p>\n",
        "    </li>\n",
        "    <li>\n",
        "        <p><strong>model_evaluate()</strong></p>\n",
        "        <p style=\"margin-left: 20px; font-size: 14px;\">\n",
        "        <ul style=\"margin-left: 20px; font-size: 14px;\">\n",
        "            <li>Shows training history</li>\n",
        "            <li>Plots training accuracy and loss</li>\n",
        "            <li>Evaluates the model on test data</li>\n",
        "            <li>Shows a classification report</li>\n",
        "            <li>Shows the confusion matrix</li>\n",
        "        </ul>\n",
        "        </p>\n",
        "    </li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUrR8ZC2gLq1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Resizing, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, InputLayer, RandomRotation, RandomFlip, RandomTranslation, RandomZoom, BatchNormalization, Rescaling, Dropout, Conv2D, MaxPool2D, Flatten, Normalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import zipfile\n",
        "import itertools\n",
        "import shutil\n",
        "\n",
        "def get_dataset(datasets):\n",
        "    os.environ['KAGGLE_USERNAME'] = 'mohamedadel452'\n",
        "    os.environ['KAGGLE_KEY'] = 'a6ea873bc8a4c8196d2683d147696840'\n",
        "    for dataset in datasets:\n",
        "        !kaggle datasets download -d {dataset}\n",
        "        with zipfile.ZipFile('/content/' + dataset.split('/')[1] + '.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/' + dataset.split('/')[1])\n",
        "\n",
        "def convert_to_tflite(saved_model_dir):\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "  with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "def delete_directory(dirs):\n",
        "    try:\n",
        "      for dir in dirs:\n",
        "        shutil.rmtree(dir)\n",
        "        print(f\"Directory deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def import_data(Image_directory, classes):\n",
        "    chunks = ['train_data','validation_data','test_data']\n",
        "\n",
        "    batch_size = 128\n",
        "\n",
        "    train_data = image_dataset_from_directory(Image_directory + '/' + chunks[0], labels = \"inferred\", label_mode = \"int\",\n",
        "          seed=123, shuffle = True, image_size=(128, 128), batch_size=batch_size)\n",
        "\n",
        "    val_data = image_dataset_from_directory(Image_directory + '/'+ chunks[1], labels = \"inferred\", label_mode = \"int\",\n",
        "      seed=123, shuffle = True, image_size=(128, 128), batch_size=batch_size)\n",
        "\n",
        "    test_data = image_dataset_from_directory(Image_directory + '/' + chunks[2], labels = \"inferred\", label_mode = \"int\",\n",
        "          seed=123, shuffle = True, image_size=(128, 128), batch_size=batch_size)\n",
        "\n",
        "\n",
        "    #Train, Test, and Validation Split adjustment\n",
        "    train_n_batches = tf.data.experimental.cardinality(train_data).numpy()\n",
        "\n",
        "    val_n_batches = tf.data.experimental.cardinality(val_data).numpy()\n",
        "    test_n_batches = tf.data.experimental.cardinality(test_data).numpy()\n",
        "\n",
        "    addition = int(train_n_batches * 0.05)\n",
        "\n",
        "    val_data = val_data.concatenate(train_data.take(addition))\n",
        "    test_data = test_data.concatenate(train_data.take(addition))\n",
        "\n",
        "\n",
        "    #Normalization layer for Basic CNN\n",
        "    Normalization_layer = Normalization(axis = -1)\n",
        "    train_image_data = train_data.map(lambda x , y: x)\n",
        "    val_image_data = val_data.map(lambda x , y: x)\n",
        "    data_0 = train_image_data.concatenate(val_image_data)\n",
        "\n",
        "    Normalization_layer.adapt(data_0)\n",
        "\n",
        "    return train_data, val_data, test_data, classes, Normalization_layer\n",
        "\n",
        "def model_and_history_dir(model_name, model_n, runs = [0]):\n",
        "\n",
        "    save_dir = '/content/model_' + str(model_n) + '_' + model_name + '/'\n",
        "\n",
        "    history_dir = '/content/model_' + str(model_n) + '_' + model_name + '_history/'\n",
        "\n",
        "    if not os.path.exists(history_dir):\n",
        "        os.makedirs(history_dir)\n",
        "\n",
        "    model_history_dir = []\n",
        "\n",
        "    for run_n in runs:\n",
        "      model_history_dir.append(history_dir + 'model_' + str(model_n) + '_' + model_name + '_training_history' + '_'  + str(run_n) + '.csv' )\n",
        "\n",
        "    return save_dir, model_history_dir\n",
        "\n",
        "def restore_model(save_dir, model):\n",
        "    if os.path.exists(save_dir):\n",
        "      model = load_model(save_dir)\n",
        "    return model\n",
        "\n",
        "def model_train(data, n_classes, n_epochs, model, model_n, lr = .001, run_n = [0], Normalize_bool = True, pt_model_w = None, tl_mode = 0, bn_train = False, bn_layer = None, pt_model_ftl_perc = None):\n",
        "\n",
        "    train_data, val_data, test_data, classes, Normalization_layer = data\n",
        "    image_size = (128,128)\n",
        "    model_name = model[0]\n",
        "    pt_model = model[1]\n",
        "\n",
        "    model = Sequential ([Resizing(*image_size)])\n",
        "\n",
        "    if Normalize_bool:\n",
        "      model.add(Normalization_layer)\n",
        "\n",
        "    model.add(RandomFlip(\"horizontal_and_vertical\"))\n",
        "    model.add(RandomRotation(0.2))\n",
        "    model.add(RandomZoom(height_factor=(-0.1, 0.1)))\n",
        "\n",
        "    if pt_model:\n",
        "\n",
        "        pt_model = pt_model(include_top=False, weights = pt_model_w, input_shape = (*image_size,3))\n",
        "\n",
        "        if tl_mode == 0:\n",
        "            pt_model.trainable = False\n",
        "\n",
        "        elif tl_mode == 1:\n",
        "            pt_model.trainable = True\n",
        "            fine_tune_at = .3 * len(pt_model.layers)\n",
        "            if pt_model_ftl_perc:\n",
        "                fine_tune_at = int(pt_model_ftl_perc * len(pt_model.layers))\n",
        "            for layer in pt_model.layers[:fine_tune_at]:\n",
        "                layer.trainable = False\n",
        "\n",
        "        elif tl_mode == 2:\n",
        "            pt_model.trainable = True\n",
        "\n",
        "        if bn_layer:\n",
        "          for layer in pt_model.layers[bn_layer:]:\n",
        "            if isinstance(layer, BatchNormalization):\n",
        "              layer.trainable = bn_train\n",
        "        else:\n",
        "          for layer in pt_model.layers:\n",
        "            if isinstance(layer, BatchNormalization):\n",
        "              layer.trainable = bn_train\n",
        "\n",
        "        model.add(pt_model)\n",
        "\n",
        "    else:\n",
        "\n",
        "        for layer in [\n",
        "            Conv2D(16, kernel_size = (3,3), input_shape = (*image_size, 3), activation = 'relu', padding = 'same'),\n",
        "            Conv2D(32, kernel_size = (3,3), activation = 'relu'),\n",
        "            MaxPool2D(pool_size = (2,2)),\n",
        "            Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n",
        "            Conv2D(64, kernel_size = (3,3), activation = 'relu'),\n",
        "            MaxPool2D(pool_size = (2,2), padding = 'same'),\n",
        "        ]:\n",
        "          model.add(layer)\n",
        "\n",
        "    for layer in [GlobalAveragePooling2D(),\n",
        "            Dense(2048, activation='relu'),\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dense(n_classes)]:\n",
        "            model.add(layer)\n",
        "\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5,\n",
        "                                            min_lr=0.0001, cooldown=2)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate = lr, amsgrad=True,),\n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[SparseCategoricalAccuracy(name='accuracy')]\n",
        "              )\n",
        "\n",
        "\n",
        "    save_dir, model_history_dir = model_and_history_dir(model_name, model_n, run_n)\n",
        "\n",
        "    model_history_dir = model_history_dir[0]\n",
        "\n",
        "    model = restore_model(save_dir, model)\n",
        "\n",
        "    history = model.fit(train_data,\n",
        "                        epochs=n_epochs,\n",
        "                        validation_data=val_data,\n",
        "                        callbacks=[learning_rate_reduction]\n",
        "    )\n",
        "\n",
        "    model.save(save_dir)\n",
        "\n",
        "    pd.DataFrame(history.history).to_csv(model_history_dir, index=True)\n",
        "\n",
        "def model_evaluate(data, model_name, model_n, runs = [0]):\n",
        "    _, _, test_data, classes, _ = data\n",
        "\n",
        "    checkpointdir, model_history_dir = model_and_history_dir(model_name, model_n, runs)\n",
        "\n",
        "    model = load_model(checkpointdir)\n",
        "\n",
        "    histories = []\n",
        "\n",
        "    for history in runs:\n",
        "      histories.append( pd.read_csv(model_history_dir[history]) )\n",
        "\n",
        "    history = pd.concat(histories)\n",
        "    history = history.reset_index(drop=True)\n",
        "\n",
        "    test_images , test_labels = zip(*list(test_data.map(lambda x, y: (x, y))))\n",
        "    test_images = np.concatenate(test_images)\n",
        "    test_labels = np.concatenate(test_labels)\n",
        "\n",
        "    #Showing training history\n",
        "    history_show = history.sort_values(by=['accuracy'], ascending=False).drop(['Unnamed: 0'], axis = 1)\n",
        "    print('Training history','\\n', history_show.head(10),'\\n')\n",
        "\n",
        "    #Evaluating model on test data\n",
        "    loss_test, acc_test = model.evaluate(test_data, verbose=1)\n",
        "    print(\"Test: accuracy = %f  ;  loss = %f\" % (acc_test, loss_test))\n",
        "    print('\\n')\n",
        "\n",
        "    #Classification report\n",
        "    test_pred = np.argmax( model.predict(test_images), axis = 1)\n",
        "    print('\\n')\n",
        "    print(classification_report(test_labels, np.around(test_pred, decimals=0)))\n",
        "\n",
        "    #Plotting training accuracy and loss\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    t = f.suptitle(model_name + ' Performance', fontsize=12)\n",
        "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "    ax1.plot(history['accuracy'], label='Train Accuracy')\n",
        "    ax1.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_xticks(np.arange(0, len(history), 5))\n",
        "    ax1.set_ylabel('Accuracy Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('Accuracy')\n",
        "    l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "    ax2.plot(history['loss'], label='Train Loss')\n",
        "    ax2.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_xticks(np.arange(0, len(history), 5))\n",
        "    ax2.set_ylabel('Loss Value')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_title('Loss')\n",
        "    l2 = ax2.legend(loc=\"best\")\n",
        "\n",
        "    #Confusion matrix\n",
        "    confusion_mtx = cm =  confusion_matrix(test_labels, test_pred)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Test Confusion matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    print('\\n')\n",
        "\n",
        "model_names = list({'Basic_CNN': None, 'EfficientNetB0': EfficientNetB0}.items())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVpV76eogLq3"
      },
      "source": [
        "<h3>2. Model Training and Evaluation</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj025Ru7gLq3"
      },
      "source": [
        "### Model-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoE71cwLgLq3",
        "outputId": "df2f8982-dc42-4f17-c1a6-c208cc72247f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedadel452/model-4-data-preprocessed\n",
            "License(s): unknown\n",
            "Downloading model-4-data-preprocessed.zip to /content\n",
            " 99% 519M/525M [00:03<00:00, 199MB/s]\n",
            "100% 525M/525M [00:03<00:00, 180MB/s]\n",
            "Found 167750 files belonging to 2 classes.\n",
            "Found 17060 files belonging to 1 classes.\n",
            "Found 8916 files belonging to 1 classes.\n",
            "/content/model_4_EfficientNetB0/\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1311/1311 [==============================] - 154s 107ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 43.9117 - val_accuracy: 0.3287 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "1311/1311 [==============================] - 135s 103ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 44.7133 - val_accuracy: 0.3282 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "1311/1311 [==============================] - ETA: 0s - loss: 8.7592e-04 - accuracy: 0.9997\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "1311/1311 [==============================] - 139s 106ms/step - loss: 8.7592e-04 - accuracy: 0.9997 - val_loss: 35.8497 - val_accuracy: 0.3283 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "1311/1311 [==============================] - 134s 102ms/step - loss: 2.3861e-04 - accuracy: 0.9999 - val_loss: 37.9440 - val_accuracy: 0.3282 - lr: 5.0000e-04\n",
            "Epoch 5/10\n",
            "1311/1311 [==============================] - 136s 104ms/step - loss: 2.5986e-04 - accuracy: 0.9999 - val_loss: 40.2916 - val_accuracy: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 6/10\n",
            "1311/1311 [==============================] - ETA: 0s - loss: 1.3360e-04 - accuracy: 0.9999\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1311/1311 [==============================] - 136s 104ms/step - loss: 1.3360e-04 - accuracy: 0.9999 - val_loss: 45.2227 - val_accuracy: 0.3280 - lr: 5.0000e-04\n",
            "Epoch 7/10\n",
            "1311/1311 [==============================] - 134s 102ms/step - loss: 2.6731e-05 - accuracy: 1.0000 - val_loss: 47.0210 - val_accuracy: 0.3281 - lr: 2.5000e-04\n",
            "Epoch 8/10\n",
            "1311/1311 [==============================] - 136s 103ms/step - loss: 1.9814e-04 - accuracy: 0.9999 - val_loss: 46.9471 - val_accuracy: 0.3279 - lr: 2.5000e-04\n",
            "Epoch 9/10\n",
            "1311/1311 [==============================] - ETA: 0s - loss: 1.1740e-04 - accuracy: 1.0000\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "1311/1311 [==============================] - 141s 107ms/step - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 46.0521 - val_accuracy: 0.3280 - lr: 2.5000e-04\n",
            "Epoch 10/10\n",
            "1311/1311 [==============================] - 136s 104ms/step - loss: 6.2537e-05 - accuracy: 1.0000 - val_loss: 47.2234 - val_accuracy: 0.3280 - lr: 1.2500e-04\n",
            "Epoch 1/20\n",
            "1311/1311 [==============================] - 153s 111ms/step - loss: 2.4799e-04 - accuracy: 0.9999 - val_loss: 61.1960 - val_accuracy: 0.3279 - lr: 1.2500e-04\n",
            "Epoch 2/20\n",
            " 227/1311 [====>.........................] - ETA: 1:40 - loss: 2.0219e-05 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "#Model_4\n",
        "datasets = ['mohamedadel452/model-4-data-preprocessed']\n",
        "Image_directory  = '/content/model-4-data-preprocessed'\n",
        "classes = {0:\"0\", 1:\"1\"}\n",
        "model = model_names[1]\n",
        "model_n = 4\n",
        "n_classes = len(classes.items())\n",
        "\n",
        "get_dataset(datasets)\n",
        "data = import_data(Image_directory, classes)\n",
        "saved_model_dir = model_and_history_dir(model[0], str(model_n), runs = [0])[0]\n",
        "print(saved_model_dir)\n",
        "model_train(data, n_classes, n_epochs = 10, model = model, model_n = model_n, lr = .001, run_n = [0], Normalize_bool = False, pt_model_w = 'imagenet', tl_mode = 0, bn_train = False, bn_layer = None)\n",
        "model_train(data, n_classes, n_epochs = 20, model = model, model_n = model_n, lr = .001, run_n = [1], Normalize_bool = False, pt_model_w = 'imagenet', tl_mode = 1, bn_train = False, pt_model_ftl_perc = .3)\n",
        "\n",
        "model_evaluate(data, model[0], runs = [0, 1])\n",
        "convert_to_tflite(saved_model_dir)\n",
        "shutil.make_archive('model_output', 'zip', saved_model_dir)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4431066,
          "sourceId": 7609931,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30646,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}